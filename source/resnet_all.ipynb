{"cells":[{"cell_type":"markdown","metadata":{"id":"AmlSpqYyu8ze"},"source":["\n","\n","# Drowsiness Detection ResNet\n","\n","a convolutional neural network with a ResNet architecture [(He et al, 2016)](https://arxiv.org/abs/1512.03385)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lO4g0RQOu8zt"},"outputs":[],"source":["skip_training = False  # Set this flag to True before validation and submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Cp1fLOalJ-4"},"outputs":[],"source":["!pip install google"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ut38RaKMD0Ee"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/DrowsinessDetection/source')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xd2TjIFKu8zy"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import tools\n","import tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEIaHJd6u8zz"},"outputs":[],"source":["# When running on your own computer, you can specify the data directory by:\n","data_dir = tools.select_data_dir('/content/drive/MyDrive/DrowsinessDetection')\n","source_dir = '/content/drive/MyDrive/DrowsinessDetection/source'\n","print(data_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vP7v4dg1u8z3"},"outputs":[],"source":["# Select the device for training (use GPU if you have one)\n","device = torch.device('cuda:0')\n","#device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzTRfcyp3b8_"},"outputs":[],"source":["print(data_dir)\n","data_dir1 = os.path.join(data_dir,'data/all_data')\n","print(data_dir1)"]},{"cell_type":"markdown","metadata":{"id":"ebPdW6d9r6oW"},"source":["## Combined Dataset\n","\n","Classes 'closed_eyes', 'open_eyes', 'alert', 'non_vigilant', 'tired', 'no_yawn' , 'yawn'' \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDUj-9hC1d3r"},"outputs":[],"source":["data_transform = transforms.Compose([\n","        transforms.Resize((28,28)),    \n","        transforms.Grayscale(),               \n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n","    ])\n","\n","all_dataset = datasets.ImageFolder(data_dir1, transform=data_transform)\n","\n","print(len(all_dataset))\n","trainset, testset = torch.utils.data.random_split(all_dataset, [len(all_dataset)-780, 780])\n","\n","classes = ['alert','closed_eyes', 'no_yawn', 'non_vigilant', 'open_eyes', 'tired', 'yawn']\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)\n","\n","print(len(trainset))\n","print(len(testset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bcXw1UncXli"},"outputs":[],"source":["#Plot a few images in greyscale\n","images, labels = iter(trainloader).next()\n","#tests.plot_images(images[:8], n_rows=2)\n","print(images[0].shape)"]},{"cell_type":"markdown","metadata":{"id":"OCAy8Zr4u80j"},"source":["## ResNet\n","\n","We create a network with an architecure inspired by [ResNet](https://arxiv.org/pdf/1512.03385.pdf).\n","\n","### ResNet block\n","Our ResNet consists of blocks with two convolutional layers and a skip connection.\n","\n","In the most general case, our implementation should have:\n","\n","<img src=\"https://drive.google.com/uc?id=1yHFVSWDnE_4N8zjLZtNIFCZ374L887rh\" width=220 style=\"float: right;\">\n","\n","\n","* Two convolutional layers with:\n","    * 3x3 kernel\n","    * no bias terms\n","    * padding with one pixel on both sides\n","    * 2d batch normalization after each convolutional layer.\n","\n","* **The first convolutional layer also (optionally) has:**\n","    * different number of input channels and output channels\n","    * change of the resolution with stride.\n","\n","* The skip connection:\n","    * simply copies the input if the resolution and the number of channels do not change.\n","    * if either the resolution or the number of channels change, the skip connection should have one convolutional layer with:\n","        * 1x1 convolution **without bias**\n","        * change of the resolution with stride (optional)\n","        * different number of input channels and output channels (optional)\n","    * if either the resolution or the number of channels change, the 1x1 convolutional layer is followed by 2d batch normalization.\n","\n","* The ReLU nonlinearity is applied after the first convolutional layer and at the end of the block.\n","\n","<div class=\"alert alert-block alert-warning\">\n","<b>Note:</b> Batch normalization is expected to be right after a convolutional layer.\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"2GZcivLZu80o"},"source":["<img src=\"https://drive.google.com/uc?id=1M_PC7w7mRVrp8bbW4hIUpNH67iYJtTWQ\" width=650 style=\"float: top;\">\n","\n","\n","\n","The implementation should also handle specific cases such as:\n","\n","Left: The number of channels and the resolution do not change.\n","There are no computations in the skip connection.\n","\n","Middle: The number of channels changes, the resolution does not change.\n","\n","Right: The number of channels does not change, the resolution changes."]},{"cell_type":"markdown","metadata":{"id":"J4s-Y_4qu80p"},"source":["Your task is to implement this block. You should use the implementations of layers in `nn.Conv2d`, `nn.BatchNorm2d` as the tests rely on those implementations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2pa7h8ou80s"},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        \"\"\"\n","        Args:\n","          in_channels (int):  Number of input channels.\n","          out_channels (int): Number of output channels.\n","          stride (int):       Controls the stride.\n","        \"\"\"\n","        super(Block, self).__init__()\n","        \n","        self.block_layers = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels , 3, stride, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","        \n","        # YOUR CODE HERE\n","        if in_channels == out_channels:\n","            if stride != 1:\n","                #print(\"Stride > 1, same channels  \", stride)\n","                self.block_skip = nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels , 1, stride, bias=False),\n","                    nn.BatchNorm2d(out_channels)\n","                )\n","            else:\n","                self.block_skip = nn.Sequential()\n","        else:\n","            #print(\"Unequal channels  \", stride)\n","            self.block_skip = nn.Sequential(    \n","                nn.Conv2d(in_channels, out_channels , 1, stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","            \n","        self.block_relu = nn.Sequential(\n","            nn.ReLU()\n","        )\n","        \n","    def forward(self, x):\n","        # YOUR CODE HERE\n","        #print(\"Input :\" , x.shape)\n","        y = self.block_layers(x)\n","        #print(\"Layers :\" , y.shape)\n","        a = self.block_skip(x)\n","        #print(\"Skip :\" , a.shape)\n","        y = y + a\n","        #print(\"Before relu :\" , y.shape)\n","        y = self.block_relu(y)\n","        #print(\"After relu :\" , y.shape)\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtSIOFETu80w"},"outputs":[],"source":["\n","\n","\n","def test_Block_shapes():\n","\n","    # The number of channels and resolution do not change\n","    batch_size = 20\n","    x = torch.zeros(batch_size, 16, 28, 28)\n","    block = Block(in_channels=16, out_channels=16)\n","    y = block(x)\n","    assert y.shape == torch.Size([batch_size, 16, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n","\n","    # Increase the number of channels\n","    block = Block(in_channels=16, out_channels=32)\n","    y = block(x)\n","    assert y.shape == torch.Size([batch_size, 32, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n","\n","    # Decrease the resolution\n","    block = Block(in_channels=16, out_channels=16, stride=2)\n","    y = block(x)\n","    assert y.shape == torch.Size([batch_size, 16, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n","\n","    # Increase the number of channels and decrease the resolution\n","    block = Block(in_channels=16, out_channels=32, stride=2)\n","    y = block(x)\n","    assert y.shape == torch.Size([batch_size, 32, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n","\n","    print('Success')\n","\n","test_Block_shapes()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwOdbMYwu80z"},"outputs":[],"source":["tests.test_Block(Block)\n","tests.test_Block_relu(Block)\n","tests.test_Block_batch_norm(Block)"]},{"cell_type":"markdown","metadata":{"id":"_6VSah8ru802"},"source":["### Group of blocks\n","\n","ResNet consists of several groups of blocks. The first block in a group may change the number of channels (often multiples the number by 2) and subsample (using strides).\n","\n","<img src=\"https://drive.google.com/uc?id=1EhVWIAmPSgook2W_m1l_Ig1PmYRwmxmr\n","\" width=200 style=\"float: right;\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnRp08wQu806"},"outputs":[],"source":["# We implement a group of blocks in this cell\n","class GroupOfBlocks(nn.Module):\n","    def __init__(self, in_channels, out_channels, n_blocks, stride=1):\n","        super(GroupOfBlocks, self).__init__()\n","\n","        first_block = Block(in_channels, out_channels, stride)\n","        other_blocks = [Block(out_channels, out_channels) for _ in range(1, n_blocks)]\n","        self.group = nn.Sequential(first_block, *other_blocks)\n","\n","    def forward(self, x):\n","        return self.group(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZR-fd5tru808"},"outputs":[],"source":["# Let's print a block\n","group = GroupOfBlocks(in_channels=10, out_channels=20, n_blocks=3)\n","print(group)"]},{"cell_type":"markdown","metadata":{"id":"Anp0EtBZu80_"},"source":["### ResNet\n","\n","Next we implement a ResNet with the following architecture. It contains three groups of blocks, each group having two basic blocks.\n","\n","<img src=\"https://drive.google.com/uc?id=1th0iWvYPHjW9eh5O6-Hqu1LOpg4urwxZ\" width=200 style=\"float: left;\">\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BM2rSzG0u81B"},"source":["The cell below contains the implementation of our ResNet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHk-3UA1u81E"},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, n_blocks, n_channels=64, num_classes=7):\n","        \"\"\"\n","        Args:\n","          n_blocks (list):   A list with three elements which contains the number of blocks in \n","                             each of the three groups of blocks in ResNet.\n","                             For instance, n_blocks = [2, 4, 6] means that the first group has two blocks,\n","                             the second group has four blocks and the third one has six blocks.\n","          n_channels (int):  Number of channels in the first group of blocks.\n","          num_classes (int): Number of classes.\n","        \"\"\"\n","        assert len(n_blocks) == 3, \"The number of groups should be three.\"\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=5, stride=1, padding=2, bias=False)\n","        self.bn1 = nn.BatchNorm2d(n_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.group1 = GroupOfBlocks(n_channels, n_channels, n_blocks[0])\n","        self.group2 = GroupOfBlocks(n_channels, 2*n_channels, n_blocks[1], stride=2)\n","        self.group3 = GroupOfBlocks(2*n_channels, 4*n_channels, n_blocks[2], stride=2)\n","\n","        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1)\n","        self.fc = nn.Linear(4*n_channels, num_classes)\n","\n","        # Initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, np.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, verbose=False):\n","        \"\"\"\n","        Args:\n","          x of shape (batch_size, 1, 28, 28): Input images.\n","          verbose: True if you want to print the shapes of the intermediate variables.\n","        \n","        Returns:\n","          y of shape (batch_size, 10): Outputs of the network.\n","        \"\"\"\n","        if verbose: print(x.shape)\n","        x = self.conv1(x)\n","        if verbose: print('conv1:  ', x.shape)\n","        x = self.bn1(x)\n","        if verbose: print('bn1:    ', x.shape)\n","        x = self.relu(x)\n","        if verbose: print('relu:   ', x.shape)\n","        x = self.maxpool(x)\n","        if verbose: print('maxpool:', x.shape)\n","\n","        x = self.group1(x)\n","        if verbose: print('group1: ', x.shape)\n","        x = self.group2(x)\n","        if verbose: print('group2: ', x.shape)\n","        x = self.group3(x)\n","        if verbose: print('group3: ', x.shape)\n","\n","        x = self.avgpool(x)\n","        if verbose: print('avgpool:', x.shape)\n","\n","        x = x.view(-1, self.fc.in_features)\n","        if verbose: print('x.view: ', x.shape)\n","        x = self.fc(x)\n","        if verbose: print('out:    ', x.shape)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yhulScSu81I"},"outputs":[],"source":["def test_ResNet_shapes():\n","    # Create a network with 2 block in each of the three groups\n","    n_blocks = [2, 2, 2]  # number of blocks in the three groups\n","    net = ResNet(n_blocks, n_channels=10)\n","    net.to(device)\n","\n","    # Feed a batch of images from the training data to test the network\n","    with torch.no_grad():\n","        images, labels = iter(trainloader).next()\n","        images = images.to(device)\n","        print('Shape of the input tensor:', images.shape)\n","\n","        y = net.forward(images, verbose=True)\n","        print(y.shape)\n","        assert y.shape == torch.Size([trainloader.batch_size, 7]), \"Bad shape of y: y.shape={}\".format(y.shape)\n","\n","    print('Success')\n","\n","test_ResNet_shapes()"]},{"cell_type":"markdown","metadata":{"id":"kCeyaCN4u81K"},"source":["# Train the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMNT-8vhu81M"},"outputs":[],"source":["# This function computes the accuracy on the test dataset\n","def compute_accuracy(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return correct / total"]},{"cell_type":"markdown","metadata":{"id":"__ieRggAu81N"},"source":["### Training loop\n","\n","In the cell below, implement the training loop. The recommended hyperparameters:\n","* Adam optimizer with learning rate 0.01.\n","* Cross-entropy loss. Note that we did not use softmax nonlinearity in the final layer of our network. Therefore, we need to use a loss function with log_softmax implemented, such as [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss).\n","* Number of epochs: 10\n","\n","We recommend you to use function `compute_accuracy()` defined above to track the accaracy during training. The test accuracy should be above 0.9.\n","\n","**Note: function `compute_accuracy()` sets the network into the evaluation mode which changes the way the batch statistics are computed in batch normalization. You need to set the network into the training mode (by calling `net.train()`) when you want to perform training.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u830o1Aju81Q"},"outputs":[],"source":["# Create the network\n","n_blocks = [2, 2, 2]  # number of blocks in the three groups\n","net = ResNet(n_blocks, n_channels=16)\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aBQCPdhu81c"},"outputs":[],"source":["if not skip_training:\n","\n","    # YOUR CODE HERE\n","    iteration=[]\n","    train_accu=[]\n","    losses=[]\n","    epochs_arr=[]\n","    train_accu_per_epoch=[]\n","    loss_per_epoch=[]\n","    optimizer = optim.Adam(net.parameters(), lr=0.01)\n","    criterion = nn.CrossEntropyLoss()\n","    epochs = 30\n","    net.train()\n","    for epoch in range(epochs):\n","        print(\"Epoch number:  \", epoch)\n","        for i, data in enumerate(trainloader, 0):\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            output = net(images)\n","            output.to(device)\n","            loss = criterion(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","            iteration.append(i)\n","            train_accu.append(compute_accuracy(net,testloader))\n","            losses.append(loss)\n","            if i % 32 == 31:\n","                acc = compute_accuracy(net, testloader)\n","                print(\"Accuracy:  \", acc)\n","                net.train()\n","        epochs_arr.append(epoch)\n","        train_accu_per_epoch.append(acc)\n","        loss_per_epoch.append(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GN-76NSLGy2d"},"outputs":[],"source":["print(iteration)\n","print(train_accu)\n","print(losses)\n","print(epochs_arr)\n","print(train_accu_per_epoch)\n","print(loss_per_epoch)\n","\n","zipped = zip(iteration, train_accu,losses,epochs_arr, train_accu_per_epoch, loss_per_epoch )\n","\n","np.savetxt('training_data.csv', zipped, fmt='%i,%i,%i,%i,%i,%i')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3d-Cxg_cu81l"},"outputs":[],"source":["# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n","# Set confirm=False if you do not want to be asked for confirmation before saving.\n","if not skip_training:\n","  path = os.path.join(source_dir, 'resnet_all.pth')\n","  tools.save_model(net, path, confirm=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1H4aaQTu81t"},"outputs":[],"source":["if skip_training:\n","    net = ResNet(n_blocks, n_channels=16)\n","    tools.load_model(net, 'resnet_all.pth', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiBcXd7Tu81v"},"outputs":[],"source":["# Compute the accuracy on the test set\n","accuracy = compute_accuracy(net, testloader)\n","print('Accuracy of the network on the test images: %.3f' % accuracy)\n","n_blocks = sum(type(m) == Block for _, m in net.named_modules())\n","assert n_blocks == 6, f\"Wrong number ({n_blocks}) of blocks used in the network.\"\n","\n","assert accuracy > 0.9, \"Poor accuracy ({:.3f})\".format(accuracy)\n","print('Success')"]},{"cell_type":"markdown","metadata":{"id":"dUJ49BSV2cb_"},"source":["# Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8enAn6lV2bx1"},"outputs":[],"source":["from matplotlib import pyplot as plt \n","# for reference :- https://howtothink.readthedocs.io/en/latest/PvL_H.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_jDdZto2ePO"},"outputs":[],"source":["plt.plot([0.1, 0.2, 0.3, 0.4], [1, 2, 3, 4], label='first plot')\n","plt.plot([0.1, 0.2, 0.3, 0.4], [1, 4, 9, 16], label='second plot')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lyuNLCB2sMg"},"outputs":[],"source":["plt.plot([0.1, 0.2, 0.3, 0.4], [1, 2, 3, 4])\n","plt.plot([0.1, 0.2, 0.3, 0.4], [1, 4, 9, 16])\n","plt.xlabel(\"Time (s)\")\n","plt.ylabel(\"Scale (Bananas)\")\n","plt.xlim(0, 1)\n","plt.ylim(-5, 20)"]},{"cell_type":"markdown","metadata":{"id":"Wt5HcVJ8yQV_"},"source":["# Hypterparameter tuning "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxgGKitdyPsK"},"outputs":[],"source":["!pip install ray"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23oTRG9xyT03"},"outputs":[],"source":["from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uo65GpUyYaY"},"outputs":[],"source":["# 1. I have to make a function for data loading\n","def load_data(data_dir1):\n","  data_transform = transforms.Compose([\n","        transforms.Resize((28,28)),    \n","        transforms.Grayscale(),               \n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n","    ])\n","  all_dataset = datasets.ImageFolder(data_dir1, transform=data_transform)\n","  print(len(all_dataset))\n","  trainset, testset = torch.utils.data.random_split(all_dataset, [len(all_dataset)-780, 780])\n","  classes = ['alert','closed_eyes', 'no_yawn', 'non_vigilant', 'open_eyes', 'tired', 'yawn']\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)\n","\n","  print(len(trainset))\n","  print(len(testset))\n","  return trainloader,testloader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqtcAF4PyaAP"},"outputs":[],"source":["# 2. I have to make a configurable net, which is already there ResNet()\n","def train_Res(config, checkpoint_dir=None, data_dir=None):\n","    net = ResNet(config[\"n_blocks\"],n_channels=16)\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda:0\"\n","        if torch.cuda.device_count() > 1:\n","            net = nn.DataParallel(net)\n","    net.to(device)\n","\n","    if checkpoint_dir:\n","        model_state, optimizer_state = torch.load(\n","            os.path.join(checkpoint_dir, \"checkpoint\"))\n","        net.load_state_dict(model_state)\n","        optimizer.load_state_dict(optimizer_state)\n","\n","    trainloader, valloader = load_data(data_dir)\n","    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n","    criterion = nn.CrossEntropyLoss()\n","    epochs = 30\n","    net.train()\n","    for epoch in range(epochs):  # loop over the dataset multiple times\n","        running_loss = 0.0\n","        epoch_steps = 0\n","        print(\"Epoch number:  \", epoch)\n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            outputs.to(device)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            epoch_steps += 1\n","            if i % 2000 == 1999:  # print every 2000 mini-batches\n","                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                                running_loss / epoch_steps))\n","                running_loss = 0.0\n","\n","        # Validation loss\n","        val_loss = 0.0\n","        val_steps = 0\n","        total = 0\n","        correct = 0\n","        for i, data in enumerate(valloader, 0):\n","            with torch.no_grad():\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = net(inputs)\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.cpu().numpy()\n","                val_steps += 1\n","\n","        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","            path = os.path.join(checkpoint_dir, \"checkpoint\")\n","            torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n","    print(\"Finished Training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqWKLlDwycAT"},"outputs":[],"source":["def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n","    data_dir = tools.select_data_dir('/content/drive/MyDrive/DrowsinessDetection')\n","    load_data(data_dir)\n","    config = {\n","        \"lr\": tune.loguniform(1e-4, 1e-1),\n","        \"batch_size\": tune.choice([2, 4, 8, 16])\n","    }\n","    scheduler = ASHAScheduler(\n","        metric=\"loss\",\n","        mode=\"min\",\n","        max_t=max_num_epochs,\n","        grace_period=1,\n","        reduction_factor=2)\n","    reporter = CLIReporter(\n","        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n","        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n","    result = tune.run(\n","        partial(train_Res, data_dir=data_dir),\n","        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n","        config=config,\n","        num_samples=num_samples,\n","        scheduler=scheduler,\n","        progress_reporter=reporter)\n","\n","    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","    print(\"Best trial config: {}\".format(best_trial.config))\n","    print(\"Best trial final validation loss: {}\".format(\n","        best_trial.last_result[\"loss\"]))\n","    print(\"Best trial final validation accuracy: {}\".format(\n","        best_trial.last_result[\"accuracy\"]))\n","\n","    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda:0\"\n","        if gpus_per_trial > 1:\n","            best_trained_model = nn.DataParallel(best_trained_model)\n","    best_trained_model.to(device)\n","\n","    best_checkpoint_dir = best_trial.checkpoint.value\n","    model_state, optimizer_state = torch.load(os.path.join(\n","        best_checkpoint_dir, \"checkpoint\"))\n","    best_trained_model.load_state_dict(model_state)\n","\n","    test_acc = test_accuracy(best_trained_model, device)\n","    print(\"Best trial test set accuracy: {}\".format(test_acc))\n","\n","\n","if __name__ == \"__main__\":\n","    # You can change the number of GPUs per trial here:\n","    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"resnet_all.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
